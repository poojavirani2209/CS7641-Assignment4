{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mdptoolbox-hiive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAwUEll9tnXk",
        "outputId": "159ecb1b-a422-407c-fa3f-0515661a7f86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mdptoolbox-hiive\n",
            "  Downloading mdptoolbox-hiive-4.0.3.1.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mdptoolbox-hiive) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mdptoolbox-hiive) (1.11.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from mdptoolbox-hiive) (0.25.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from mdptoolbox-hiive) (7.34.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from mdptoolbox-hiive) (3.2.1)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from mdptoolbox-hiive) (1.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->mdptoolbox-hiive) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->mdptoolbox-hiive) (0.0.8)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->mdptoolbox-hiive)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->mdptoolbox-hiive) (4.8.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot->mdptoolbox-hiive) (3.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->mdptoolbox-hiive) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->mdptoolbox-hiive) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mdptoolbox-hiive) (0.2.10)\n",
            "Building wheels for collected packages: mdptoolbox-hiive\n",
            "  Building wheel for mdptoolbox-hiive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mdptoolbox-hiive: filename=mdptoolbox_hiive-4.0.3.1-py3-none-any.whl size=35120 sha256=045ad3d3de089832cd718391a02a64d870575710e56ec5fdf22d0a4985f9a640\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/21/00/79fb3890bf11432b069070b7623416cd8b9f8501580692a52f\n",
            "Successfully built mdptoolbox-hiive\n",
            "Installing collected packages: jedi, mdptoolbox-hiive\n",
            "Successfully installed jedi-0.19.1 mdptoolbox-hiive-4.0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vg8x3WaokdUT"
      },
      "outputs": [],
      "source": [
        "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIteration, QLearning\n",
        "from hiive.mdptoolbox.example import forest\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from numpy.random import choice\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_PyhLjO-kdUU"
      },
      "outputs": [],
      "source": [
        "def mean(x, N):\n",
        "    sum = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (sum[N:] - sum[:-N]) / float(N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K7hS05yRkdUU"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(prob_matrix, reward_matrix, optimal_policy, test_count=100, gamma=0.9):\n",
        "    states = prob_matrix.shape[-1]\n",
        "    episodes = states * test_count\n",
        "    total_reward = 0\n",
        "    for state in range(states):\n",
        "        state_reward = 0\n",
        "        for state_episode in range(test_count):\n",
        "            episode_reward = 0\n",
        "            discount_rate = 1\n",
        "            while True:\n",
        "                action = optimal_policy[state]\n",
        "                probs = prob_matrix[action][state]\n",
        "                candidates = list(range(len(prob_matrix[action][state])))\n",
        "                next_state =  choice(candidates, 1, p=probs)[0]\n",
        "                reward = reward_matrix[state][action] * discount_rate\n",
        "                episode_reward += reward\n",
        "                discount_rate *= gamma\n",
        "                if next_state == 0:\n",
        "                    break\n",
        "            state_reward += episode_reward\n",
        "        total_reward += state_reward\n",
        "    return total_reward / episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mVN95Xg6kdUV"
      },
      "outputs": [],
      "source": [
        "def value_iteration(prob_matrix, reward_matrix, discount_factor=0.9, epsilons=[1e-9]):\n",
        "    data_frame = pd.DataFrame(columns=[\"Epsilon\", \"Optimal_Policy\", \"Iterations\",\n",
        "                                  \"Time taken\", \"Reward\", \"Value Function\"])\n",
        "    for e in epsilons:\n",
        "        value_itr = ValueIteration(prob_matrix, reward_matrix, gamma=discount_factor, epsilon=e, max_iter=int(1e15))\n",
        "        value_itr.run()\n",
        "        reward = evaluate_policy(prob_matrix, reward_matrix, value_itr.policy)\n",
        "        info = [float(e), value_itr.policy, value_itr.iter, value_itr.time, reward, value_itr.V]\n",
        "        data_frame.loc[len(data_frame)] = info\n",
        "    return data_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YfpuqjcMkdUW"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(prob_matrix, reward_matrix, discount_factor=0.9):\n",
        "  policy_itr = PolicyIteration(prob_matrix, reward_matrix, gamma=discount_factor, max_iter=1e6)\n",
        "  policy_itr.run()\n",
        "  policy = policy_itr.policy\n",
        "  reward = evaluate_policy(prob_matrix, reward_matrix, policy)\n",
        "  iterations = policy_itr.iter\n",
        "  time = policy_itr.time\n",
        "  return iterations, time, reward\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oECkQTffkdUe"
      },
      "outputs": [],
      "source": [
        "def q_learning(prob_matrix, reward_matrix, discount=0.9, alpha_dec=[0.99], alpha_min=[0.001],\n",
        "            epsilon=[1.0], epsilon_decay=[0.99], n_iter=[1000000]):\n",
        "    q_df = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\",\n",
        "                                 \"Epsilon\", \"Epsilon Decay\", \"Reward\",\n",
        "                                 \"Time\", \"Policy\", \"Value Function\",\n",
        "                                 \"Training Rewards\"])\n",
        "\n",
        "    count = 0\n",
        "    for i in n_iter:\n",
        "        for eps in epsilon:\n",
        "            for eps_dec in epsilon_decay:\n",
        "                for a_dec in alpha_dec:\n",
        "                    for a_min in alpha_min:\n",
        "                        q = QLearning(prob_matrix, reward_matrix, discount, alpha_decay=a_dec,\n",
        "                                      alpha_min=a_min, epsilon=eps,\n",
        "                                      epsilon_decay=eps_dec, n_iter=i)\n",
        "                        q.run()\n",
        "                        reward = evaluate_policy(prob_matrix, reward_matrix, q.policy)\n",
        "                        count += 1\n",
        "                        print(\"{}: {}\".format(count, reward))\n",
        "                        st = q.run_stats\n",
        "                        rews = [s['Reward'] for s in st]\n",
        "                        info = [i, a_dec, a_min, eps, eps_dec, reward,\n",
        "                                q.time, q.policy, q.V, rews]\n",
        "\n",
        "                        df_length = len(q_df)\n",
        "                        q_df.loc[df_length] = info\n",
        "    return q_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOREST MANAGEMENT WITH 50 STATES"
      ],
      "metadata": {
        "id": "u9BLTOr-5I2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X1rulRodkdUU"
      },
      "outputs": [],
      "source": [
        "prob_matrix, reward_matrix = forest(S=50, r1=10, r2= 5, p=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q99l-zaEkdUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ea28ce-9170-4aac-a85d-02dbc12e6afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Epsilon                                     Optimal_Policy  \\\n",
            "0  1.000000e-01  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "1  1.000000e-02  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "2  1.000000e-03  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "3  1.000000e-05  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "4  1.000000e-09  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "5  1.000000e-12  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "\n",
            "   Iterations  Time taken    Reward  \\\n",
            "0          59    0.003872  2.252649   \n",
            "1          79    0.003982  2.273600   \n",
            "2          99    0.008129  2.324879   \n",
            "3         139    0.011879  2.205875   \n",
            "4         219    0.019604  2.330401   \n",
            "5         279    0.013138  2.318898   \n",
            "\n",
            "                                      Value Function  \n",
            "0  (4.701569160732358, 5.23097346057433, 5.230973...  \n",
            "1  (4.710556185449387, 5.239434944489701, 5.23943...  \n",
            "2  (4.711643009800913, 5.2404695099104295, 5.2404...  \n",
            "3  (4.711790503732882, 5.2406112905625415, 5.2406...  \n",
            "4  (4.711792701797032, 5.240613431575217, 5.24061...  \n",
            "5  (4.711792702273076, 5.240613432045689, 5.24061...  \n"
          ]
        }
      ],
      "source": [
        "print( value_iteration(prob_matrix, reward_matrix, epsilons=[1e-1, 1e-2, 1e-3, 1e-5, 1e-9, 1e-12]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(policy_iteration(prob_matrix, reward_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMJHbQQx5Gri",
        "outputId": "b73ed66f-a87c-4bc4-9a5f-6c7b1b73f6ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26, 0.031317710876464844, 2.315741141279923)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TdFc7vmKkdUe",
        "outputId": "96f53cbd-4031-486f-9133-98c74b405a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 2.698958896143808\n",
            "2: 1.04\n",
            "3: 2.654422944691908\n",
            "4: 0.94\n",
            "5: 0.96\n",
            "6: 2.7845738650567076\n",
            "7: 0.9\n",
            "8: 2.633606735223075\n",
            "9: 2.7308972211252103\n",
            "10: 1.04\n",
            "11: 2.6349138726147725\n",
            "12: 2.674642988618572\n",
            "13: 2.68238365748513\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9fa1a662957b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m print(q_learning(prob_matrix, reward_matrix, discount=0.9, alpha_dec=[0.99, 0.999], alpha_min=[0.001, 0.0001],\n\u001b[0m\u001b[1;32m      2\u001b[0m             epsilon=[10.0, 1.0], epsilon_decay=[0.99, 0.999], n_iter=[1000000, 10000000]))\n",
            "\u001b[0;32m<ipython-input-10-ac6e36b9f57c>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(prob_matrix, reward_matrix, discount, alpha_dec, alpha_min, epsilon, epsilon_decay, n_iter)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                       \u001b[0malpha_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                       epsilon_decay=eps_dec, n_iter=i)\n\u001b[0;32m---> 17\u001b[0;31m                         \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hiive/mdptoolbox/mdp.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0mrun_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_run_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtake_run_stat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(q_learning(prob_matrix, reward_matrix, discount=0.9, alpha_dec=[0.99, 0.999], alpha_min=[0.001, 0.0001],\n",
        "            epsilon=[10.0, 1.0], epsilon_decay=[0.99, 0.999], n_iter=[1000000, 10000000]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOREST MANAGEMENT WITH 400 STATES"
      ],
      "metadata": {
        "id": "Kp0BLCum5v4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_matrix, reward_matrix = forest(S=400, r1=10, r2= 5, p=0.01)"
      ],
      "metadata": {
        "id": "YCSfHYt852NJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( value_iteration(prob_matrix, reward_matrix, epsilons=[1e-1, 1e-2, 1e-3, 1e-5, 1e-9, 1e-12]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5X4lhTX57VQ",
        "outputId": "e3adbeb7-2abd-4a71-9f30-c8ef29ad8abc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Epsilon                                     Optimal_Policy  \\\n",
            "0  1.000000e-01  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "1  1.000000e-02  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "2  1.000000e-03  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "3  1.000000e-05  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "4  1.000000e-09  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "5  1.000000e-12  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "\n",
            "   Iterations  Time taken    Reward  \\\n",
            "0          59    0.033690  1.157760   \n",
            "1          79    0.035558  1.151885   \n",
            "2          99    0.047704  1.165787   \n",
            "3         139    0.056917  1.157420   \n",
            "4         219    0.053029  1.169877   \n",
            "5         279    0.066078  1.163662   \n",
            "\n",
            "                                      Value Function  \n",
            "0  (4.701569160732358, 5.23097346057433, 5.230973...  \n",
            "1  (4.710556185449387, 5.239434944489701, 5.23943...  \n",
            "2  (4.711643009800913, 5.2404695099104295, 5.2404...  \n",
            "3  (4.711790503732882, 5.2406112905625415, 5.2406...  \n",
            "4  (4.711792701797032, 5.240613431575217, 5.24061...  \n",
            "5  (4.711792702273076, 5.240613432045689, 5.24061...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(policy_iteration(prob_matrix, reward_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHFJMVGK59aj",
        "outputId": "e77cc465-f600-420e-8d12-155c57abb2fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26, 0.2763371467590332, 1.1629042434708299)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_learning(prob_matrix, reward_matrix, discount=0.9, alpha_dec=[0.99, 0.999], alpha_min=[0.001, 0.0001],\n",
        "            epsilon=[10.0, 1.0], epsilon_decay=[0.99, 0.999], n_iter=[1000000, 10000000]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "yTLndfU55_rl",
        "outputId": "cb883482-eb50-4472-f4b6-1db4079659c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 1.049307917823982\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9fa1a662957b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m print(q_learning(prob_matrix, reward_matrix, discount=0.9, alpha_dec=[0.99, 0.999], alpha_min=[0.001, 0.0001],\n\u001b[0m\u001b[1;32m      2\u001b[0m             epsilon=[10.0, 1.0], epsilon_decay=[0.99, 0.999], n_iter=[1000000, 10000000]))\n",
            "\u001b[0;32m<ipython-input-10-ac6e36b9f57c>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(prob_matrix, reward_matrix, discount, alpha_dec, alpha_min, epsilon, epsilon_decay, n_iter)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                       epsilon_decay=eps_dec, n_iter=i)\n\u001b[1;32m     17\u001b[0m                         \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bf4fd136d039>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(prob_matrix, reward_matrix, optimal_policy, test_count, gamma)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_policy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiscount_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}